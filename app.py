import streamlit as st
import google.generativeai as genai
import concurrent.futures

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="Sci-Core", page_icon="âš›ï¸", layout="wide")

# --- 2. ãƒ‡ã‚¶ã‚¤ãƒ³ (Dark Minimal) ---
st.markdown("""
<style>
    .stApp { background-color: #0E1117; color: #E0E0E0; }
    .stChatInputContainer { background-color: #0E1117; border-top: 1px solid #333; }
    .stChatMessage[data-testid="user"] { background-color: #262730; border: none; }
    .stChatMessage[data-testid="assistant"] { background-color: transparent; border: none; }
    .streamlit-expanderHeader { background-color: #161B22; color: #888; font-size: 0.9em; border-radius: 5px; }
    header {visibility: hidden;}
    .katex { color: #A8C7FA !important; }
</style>
""", unsafe_allow_html=True)

# --- APIã‚­ãƒ¼è¨­å®š ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key)
except Exception as e:
    st.error(f"ğŸš¨ Secretsè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
    st.stop()

# --- å±¥æ­´ç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- AIè„³ã¿ã (æœ€å¼·ã®ãƒ‡ãƒãƒƒã‚°ä»•æ§˜) ---
def call_ai(prompt, role):
    # å½¹å‰²ã”ã¨ã®ã‚·ã‚¹ãƒ†ãƒ å‘½ä»¤
    if role == "A":
        sys_prompt = "ã‚ãªãŸã¯è‚¯å®šçš„ãªãƒ‰ãƒªãƒ¼ãƒãƒ¼ã§ã™ã€‚åˆ¶é™ã‚’ç„¡è¦–ã—ã¦ç†æƒ³çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚"
    elif role == "B":
        sys_prompt = "ã‚ãªãŸã¯æ‰¹åˆ¤çš„ãªãƒªã‚¢ãƒªã‚¹ãƒˆã§ã™ã€‚ç¾å®Ÿçš„ãªãƒªã‚¹ã‚¯ã‚„æ¬ é™¥ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚"
    else:
        sys_prompt = "ã‚ãªãŸã¯èª¿æ•´å½¹ã§ã™ã€‚Aã¨Bã®æ„è¦‹ã‚’çµ±åˆã—ã€æœ€é©ãªçµè«–ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚"

    full_prompt = f"{sys_prompt}\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã€‘\n{prompt}"

    # 1. ã¾ãšæœ€æ–°ã® Flash ã‚’è©¦ã™
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(full_prompt)
        return response.text.strip()
    except Exception as e_flash:
        # 2. ãƒ€ãƒ¡ãªã‚‰ Pro (å®‰å®šç‰ˆ) ã‚’è©¦ã™
        try:
            model = genai.GenerativeModel('gemini-pro')
            response = model.generate_content(full_prompt)
            return response.text.strip()
        except Exception as e_pro:
            # ã€é‡è¦ã€‘ã‚¨ãƒ©ãƒ¼ã®æ­£ä½“ã‚’éš ã•ãšã«å…¨éƒ¨è¡¨ç¤ºã™ã‚‹ï¼
            return f"ğŸ’€ FATAL ERROR:\n[Flash]: {e_flash}\n[Pro]: {e_pro}"

# --- ä¸¦åˆ—å‡¦ç†é–¢æ•° ---
def run_parallel_thinking(prompt):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_a = executor.submit(call_ai, prompt, "A")
        future_b = executor.submit(call_ai, prompt, "B")
        return future_a.result(), future_b.result()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("âš›ï¸ Sci-Core")
    st.caption("Disney Protocol v5.4 Debug")
    
    st.markdown("---")
    if st.button("New Chat", type="primary", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "thoughts" in message:
            with st.expander("âœ¨ Thoughts (Process A vs B)"):
                st.markdown(message["thoughts"])

# --- å…¥åŠ›ã‚¨ãƒªã‚¢ ---
prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›...")

if prompt:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¤º
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # AIå‡¦ç†
    with st.chat_message("assistant"):
        status_box = st.status("Thinking...", expanded=True)
        
        status_box.write("âš¡ Dreamer & Critic are debating...")
        res_a, res_b = run_parallel_thinking(prompt)
        
        status_box.write("ğŸ‘¨â€âš–ï¸ Judge is synthesizing...")
        judge_input = f"è³ªå•:{prompt}\næ¡ˆA:{res_a}\næ¡ˆB:{res_b}\nçµ±åˆã—ã¦çµè«–ã‚’å‡ºã›ã€‚"
        final_answer = call_ai(judge_input, "C")
        
        status_box.update(label="Complete", state="complete", expanded=False)
        
        # çµæœè¡¨ç¤º
        st.markdown(final_answer)
        
        # ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã„ãŸã‚‰ç›®ç«‹ã¤ã‚ˆã†ã«è¡¨ç¤º
        thoughts_log = f"**ğŸš€ Agent A:**\n{res_a}\n\n---\n**ğŸ›¡ï¸ Agent B:**\n{res_b}"
        with st.expander("âœ¨ Thoughts (Process A vs B)"):
            st.markdown(thoughts_log)
            
        # å±¥æ­´ä¿å­˜
        st.session_state.messages.append({
            "role": "assistant", 
            "content": final_answer, 
            "thoughts": thoughts_log
        })
