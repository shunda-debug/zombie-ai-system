import streamlit as st
import time
import re
from google import genai

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ‡ã‚¶ã‚¤ãƒ³æ³¨å…¥ï¼ˆã‚¹ãƒãƒ›å®Œå…¨å¯¾å¿œç‰ˆï¼‰ ---
st.set_page_config(page_title="Sci-Core AI", page_icon="âš›ï¸", layout="wide")

st.markdown("""
<style>
    /* å…¨ä½“ã®èƒŒæ™¯ã¨åŸºæœ¬ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š */
    .stApp {
        background-color: #0E1117;
        color: #FFFFFF !important;
    }
    
    /* ã‚¹ãƒãƒ›å¯¾ç­–: å…¥åŠ›æ¬„ã®è‰²ã‚’å¼·åˆ¶çš„ã«ãƒ€ãƒ¼ã‚¯ã«ã™ã‚‹ */
    .stChatInput textarea {
        background-color: #161B22 !important; /* èƒŒæ™¯ã‚’æ¿ƒã„ã‚°ãƒ¬ãƒ¼ã« */
        color: #FFFFFF !important; /* æ–‡å­—ã‚’ç™½ã« */
        caret-color: #FFFFFF !important; /* ã‚«ãƒ¼ã‚½ãƒ«ã‚‚ç™½ã« */
    }
    /* å…¥åŠ›æ¬„ã®ã‚³ãƒ³ãƒ†ãƒŠè‡ªä½“ã‚‚é»’ã */
    div[data-testid="stChatInput"] {
        background-color: #0E1117 !important;
    }

    /* æ–‡å­—ã‚’å…¨ä½“çš„ã«ãã£ãã‚Šã•ã›ã‚‹ */
    body, p, div, span, label, h1, h2, h3, h4, h5, h6 {
        color: #FFFFFF !important;
        font-weight: 500 !important;
        -webkit-font-smoothing: antialiased;
    }

    /* ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç®± */
    .stChatMessage {
        background-color: #161B22;
        border: 1px solid #30363D;
        border-radius: 10px;
        padding: 15px;
    }

    /* æ•°å¼ï¼ˆLaTeXï¼‰ã®è¨­å®š */
    .katex {
        font-size: 1.3em !important;
        color: #58A6FF !important;
    }

    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ */
    [data-testid="stSidebar"] {
        background-color: #010409;
        border-right: 1px solid #30363D;
    }
    
    /* ãƒœã‚¿ãƒ³ */
    .stButton button {
        background-color: #238636;
        color: white !important;
        border-radius: 5px;
        font-weight: bold;
        border: none;
    }
</style>
""", unsafe_allow_html=True)

# --- APIã‚­ãƒ¼ ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except:
    st.error("ğŸš¨ APIã‚­ãƒ¼è¨­å®šãŒå¿…è¦ã§ã™")
    st.stop()

client = genai.Client(api_key=api_key)

# --- 2. å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [] # éå»ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ

if "messages" not in st.session_state:
    st.session_state.messages = [] # ç¾åœ¨ã®ä¼šè©±

# --- 3. ç†ç³»ç‰¹åŒ–ã®è„³ã¿ã ---
def call_science_model(client, prompt, role="solver"):
    try:
        if role == "solver":
            sys_instruction = """
            ã‚ãªãŸã¯ä¸–ç•Œæœ€é«˜å³°ã®ç§‘å­¦æŠ€è¡“è¨ˆç®—AIã§ã™ã€‚
            æ•°å¼ã¯å¿…ãš `$$` ã§å›²ã¿ã€`\\begin{align}` ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚
            æš—ç®—ç¦æ­¢ã€‚é€”ä¸­å¼ã‚’ä¸å¯§ã«æ›¸ãã€å˜ä½ã‚’æ­£ç¢ºã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
            """
        else: # Judge
            sys_instruction = """
            ã‚ãªãŸã¯å³æ ¼ãªæ•°å­¦æŸ»èª­è€…ã§ã™ã€‚
            3ã¤ã®å›ç­”ã‚’æ¯”è¼ƒã—ã€æœ€ã‚‚æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            `\\begin{align}` ã¯ä½¿ç”¨ç¦æ­¢ã€‚ã™ã¹ã¦ã®æ•°å¼ã¯ `$$` ã¾ãŸã¯ `$` ã§å›²ã‚“ã§ãã ã•ã„ã€‚
            """
        
        res = client.models.generate_content(
            model="gemini-2.0-flash", 
            contents=prompt,
            config={"system_instruction": sys_instruction}
        )
        return res.text.strip()
    except:
        return None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("âš›ï¸ Sci-Core AI")
    st.caption("v2.3 History & Mobile")
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    col1, col2, col3 = st.columns(3)
    col1.metric("A", "ğŸŸ¢")
    col2.metric("B", "ğŸŸ¢")
    col3.metric("C", "ğŸŸ¢")
    
    st.markdown("---")
    
    # ğŸ†• æ–°ã—ã„ä¼šè©±ãƒœã‚¿ãƒ³ï¼ˆå±¥æ­´ã«ä¿å­˜ã—ã¦ã‹ã‚‰ãƒªã‚»ãƒƒãƒˆï¼‰
    if st.button("â• æ–°ã—ã„ä¼šè©±ã‚’å§‹ã‚ã‚‹", use_container_width=True):
        if st.session_state.messages:
            # ç¾åœ¨ã®ä¼šè©±ã‚’å±¥æ­´ãƒªã‚¹ãƒˆã«ä¿å­˜
            summary = st.session_state.messages[0]["content"][:20] + "..." if st.session_state.messages else "No Data"
            st.session_state.chat_history.append({"title": summary, "log": st.session_state.messages})
        # ç¾åœ¨ã®ä¼šè©±ã‚’ã‚¯ãƒªã‚¢
        st.session_state.messages = []
        st.rerun()

    st.markdown("### ğŸ“š éå»ã®ä¼šè©±å±¥æ­´")
    if not st.session_state.chat_history:
        st.caption("å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“")
    else:
        # éå»ã®ä¼šè©±ã‚’ã‚¢ã‚³ãƒ¼ãƒ‡ã‚£ã‚ªãƒ³ã§è¡¨ç¤º
        for i, chat in enumerate(reversed(st.session_state.chat_history)):
            with st.expander(f"ğŸ“ {chat['title']}"):
                for msg in chat["log"]:
                    st.text(f"{msg['role']}: {msg['content']}")

    st.info("ğŸ’¡ ã‚¹ãƒãƒ›ã®æ–¹ã¯ã€å·¦ä¸Šã®ã€Œ>ã€ã¾ãŸã¯ã€Œâ‰¡ã€ã‚’æŠ¼ã™ã¨ã“ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãŒé–‹ãã¾ã™ã€‚")

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("âš›ï¸ Sci-Core Solver")
st.markdown("#### ç©¶æ¥µã®è¨ˆç®—ç²¾åº¦ã¨ã€ç¾ã—ã„æ•°å¼è¡¨ç¤ºã€‚")

# ç¾åœ¨ã®å±¥æ­´è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "details" in message:
            with st.expander("ğŸ” è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã‚‹"):
                st.markdown(message["details"])

# è³ªå•å…¥åŠ›
question = st.chat_input("æ•°å¼ã€ç‰©ç†æ³•å‰‡ã€è¨ˆç®—å•é¡Œã‚’å…¥åŠ›...")

if question:
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state.messages.append({"role": "user", "content": question})

    with st.chat_message("assistant"):
        status = st.empty()
        status.info("âš¡ 3ã¤ã®AIè„³ãŒä¸¦åˆ—æ¼”ç®—ä¸­...")
        
        # 1. ã‚½ãƒ«ãƒãƒ¼å®Ÿè¡Œ
        res_a = call_science_model(client, question, "solver")
        res_b = call_science_model(client, question, "solver")
        res_c = call_science_model(client, question, "solver")
        
        ans_a = res_a if res_a else "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        ans_b = res_b if res_b else "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        ans_c = res_c if res_c else "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        
        # 2. æŸ»èª­
        status.info("ğŸ‘¨â€âš–ï¸ æŸ»èª­è€…ãŒæ•°å¼ã‚’æ•´å½¢ãƒ»æ¤œç®—ä¸­...")
        
        log_text = f"""
        **Solver A:** {ans_a}
        **Solver B:** {ans_b}
        **Solver C:** {ans_c}
        """

        # 3. æœ€çµ‚å›ç­”ç”Ÿæˆ
        judge_prompt = f"""
        ã€å•é¡Œã€‘{question}
        ã€è§£æ³•Aã€‘{ans_a}
        ã€è§£æ³•Bã€‘{ans_b}
        ã€è§£æ³•Cã€‘{ans_c}
        
        ä¸Šè¨˜ã‚’çµ±åˆã—ã€æ­£ã—ã„è¨ˆç®—çµæœã‚’å›ç­”ã—ã¦ãã ã•ã„ã€‚
        æ•°å¼ã¯å¿…ãš `$$` ã§å›²ã¿ã€alignç’°å¢ƒã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚
        """
        
        final_answer = call_science_model(client, judge_prompt, "judge")
        
        if final_answer:
            status.empty()
            st.markdown(final_answer)
            st.session_state.messages.append({
                "role": "assistant", 
                "content": final_answer, 
                "details": log_text
            })
        else:
            status.error("ğŸ’€ è¨ˆç®—å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
