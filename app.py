import streamlit as st
from google import genai
from PIL import Image

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š (æœ€åˆã«è¡Œã†) ---
st.set_page_config(page_title="Sci-Core AI", page_icon="âš›ï¸", layout="wide")

# --- 2. ãƒ‡ã‚¶ã‚¤ãƒ³ã®å¼·åˆ¶é©ç”¨ (CSSãƒãƒƒã‚¯) ---
# ãƒ©ã‚¤ãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’æ’é™¤ã—ã€æœ€å¼·ã®ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ã‚’å¼·åˆ¶ã—ã¾ã™
st.markdown("""
<style>
    /* å…¨ä½“ã®èƒŒæ™¯ã‚’æ¼†é»’ã« */
    .stApp {
        background-color: #050505 !important;
        color: #E0E0E0 !important;
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®èƒŒæ™¯ */
    [data-testid="stSidebar"] {
        background-color: #0F0F0F !important;
        border-right: 1px solid #333;
    }
    
    /* å…¥åŠ›æ¬„ï¼ˆLINEé¢¨ï¼‰ã®ã‚¹ã‚¿ã‚¤ãƒ«èª¿æ•´ */
    .stChatInputContainer {
        background-color: #050505 !important;
    }
    
    /* ãƒ˜ãƒƒãƒ€ãƒ¼ã®éè¡¨ç¤ºï¼ˆã‚¹ãƒƒã‚­ãƒªã•ã›ã‚‹ï¼‰ */
    header {visibility: hidden;}
    
    /* æ•°å¼ã®æ–‡å­—è‰²ï¼ˆãƒã‚ªãƒ³ãƒ–ãƒ«ãƒ¼ï¼‰ */
    .katex { color: #4DA6FF !important; }
    
    /* ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¹ãå‡ºã— */
    .stChatMessage[data-testid="user"] {
        background-color: #1E1E1E;
        border-radius: 15px;
        padding: 10px;
    }
    
    /* AIã®å¹ãå‡ºã— */
    .stChatMessage[data-testid="assistant"] {
        background-color: #000000;
        border: 1px solid #333;
        border-radius: 15px;
        padding: 10px;
    }
</style>
""", unsafe_allow_html=True)

# --- APIã‚­ãƒ¼ ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except:
    st.error("ğŸš¨ API Key Missing")
    st.stop()

client = genai.Client(api_key=api_key)

# --- å±¥æ­´ç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- AIé–¢æ•° ---
def call_science_model(client, prompt, image=None, role="solver"):
    try:
        sys_instruction = "ã‚ãªãŸã¯ç§‘å­¦æŠ€è¡“è¨ˆç®—AIã§ã™ã€‚æ•°å¼ã¯$$ã‚’ä½¿ç”¨ã—ã€è«–ç†çš„ã‹ã¤ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
        if role == "judge":
            sys_instruction = "ã‚ãªãŸã¯æŸ»èª­è€…ã§ã™ã€‚è¤‡æ•°ã®å›ç­”ã‚’çµ±åˆã—ã€å®Œç’§ãªæœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        
        contents = [prompt, image] if image else prompt
        res = client.models.generate_content(
            model="gemini-2.0-flash", 
            contents=contents,
            config={"system_instruction": sys_instruction}
        )
        return res.text.strip()
    except:
        return None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("âš›ï¸ Sci-Core")
    st.caption("Autonomous Reasoning System")
    
    st.markdown("---")
    # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«éš ã—ã¦ã‚¹ãƒƒã‚­ãƒªã•ã›ã‚‹
    st.markdown("### ğŸ“ ç”»åƒè§£æ")
    uploaded_file = st.file_uploader("Upload Image", type=["jpg", "png"], label_visibility="collapsed")
    
    st.markdown("---")
    if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚¯ãƒªã‚¢", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.markdown("## âš›ï¸ Sci-Core AI Project")
st.caption("Multi-Agent Reasoning Engine")

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if "image" in message and message["image"]:
            st.image(message["image"], width=250)
        st.markdown(message["content"])
        if "details" in message:
            with st.expander("ğŸ” æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ (3æ©Ÿã®AIã«ã‚ˆã‚‹æ¨è«–)"):
                st.markdown(message["details"])

# --- å…¥åŠ›ã‚¨ãƒªã‚¢ (æœ€æ–°ã®ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹) ---
# ã“ã‚ŒãŒã‚¹ãƒãƒ›ã§ã‚‚ä½¿ã„ã‚„ã™ã„ã€Œé€ä¿¡ãƒœã‚¿ãƒ³ä»˜ãã€ã®å…¥åŠ›æ¬„ã§ã™
prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›... (Shift+Enterã§æ”¹è¡Œ)")

if prompt:
    # ç”»åƒã®å‡¦ç†
    image = Image.open(uploaded_file) if uploaded_file else None
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¤º
    with st.chat_message("user"):
        if image: st.image(image, width=250)
        st.markdown(prompt)
    
    # å±¥æ­´ä¿å­˜
    msg_data = {"role": "user", "content": prompt}
    if image: msg_data["image"] = image
    st.session_state.messages.append(msg_data)
    
    # AIå‡¦ç†
    with st.chat_message("assistant"):
        status = st.empty()
        status.markdown("`âš¡ Sci-Core is thinking...`")
        
        # ä¸¦åˆ—å‡¦ç†é¢¨ã®æ¼”å‡º
        res_a = call_science_model(client, prompt, image, "solver")
        res_b = call_science_model(client, prompt, image, "solver")
        
        status.markdown("`ğŸ‘¨â€âš–ï¸ Judge is verifying...`")
        
        judge_prompt = f"è³ªå•: {prompt}\nå›ç­”A: {res_a}\nå›ç­”B: {res_b}\nã“ã‚Œã‚‰ã‚’çµ±åˆã—ã€æ´—ç·´ã•ã‚ŒãŸå›ç­”ã‚’ä½œæˆã›ã‚ˆã€‚"
        final_answer = call_science_model(client, judge_prompt, None, "judge")
        
        if final_answer:
            status.empty()
            st.markdown(final_answer)
            st.session_state.messages.append({
                "role": "assistant", 
                "content": final_answer,
                "details": f"**Core A:** {res_a}\n\n**Core B:** {res_b}"
            })
        else:
            status.error("System Error")
