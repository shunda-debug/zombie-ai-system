import streamlit as st
import time
import re
from google import genai
from PIL import Image

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="Sci-Core AI", page_icon="âš›ï¸", layout="wide")

# å¼·åˆ¶ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ & ã‚¹ãƒãƒ›æœ€é©åŒ– & ãƒ‡ã‚¶ã‚¤ãƒ³
st.markdown("""
<style>
    .stApp { background-color: #0E1117 !important; color: #FFFFFF !important; }
    .stChatInput textarea { background-color: #262730 !important; color: #FFFFFF !important; }
    [data-testid="stSidebar"] { background-color: #161B22 !important; }
    body, p, div, span, h1, h2, h3, li { color: #FFFFFF !important; -webkit-text-fill-color: #FFFFFF !important; }
    .katex { color: #58A6FF !important; font-size: 1.2em !important; }
    .stButton button { background-color: #238636; color: white !important; font-weight: bold; border: none; }
    
    /* ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æ ç·šã‚’è¦‹ã‚„ã™ã */
    [data-testid="stFileUploader"] {
        padding: 10px;
        border: 1px dashed #4E5359;
        border-radius: 10px;
        background-color: #161B22;
    }
</style>
""", unsafe_allow_html=True)

# --- APIã‚­ãƒ¼ ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except:
    st.error("ğŸš¨ ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼è¨­å®šãŒå¿…è¦ã§ã™")
    st.stop()

client = genai.Client(api_key=api_key)

# --- å±¥æ­´ç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- AIè„³ã¿ã (ç”»åƒå¯¾å¿œ) ---
def call_science_model(client, prompt, image=None, role="solver"):
    try:
        if role == "solver":
            sys_instruction = """
            ã‚ãªãŸã¯ä¸–ç•Œæœ€é«˜å³°ã®ç§‘å­¦æŠ€è¡“è¨ˆç®—AIã§ã™ã€‚
            æ•°å¼ã¯å¿…ãš `$$` ã§å›²ã¿ã€`\\begin{align}` ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚
            ç”»åƒãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã¯ã€ãã®ç”»åƒå†…ã®æ•°å¼ã‚„ç¾è±¡ã‚’è§£æã—ã¦ãã ã•ã„ã€‚
            æš—ç®—ç¦æ­¢ã€‚é€”ä¸­å¼ã‚’ä¸å¯§ã«æ›¸ã„ã¦ãã ã•ã„ã€‚
            """
        else: # Judge
            sys_instruction = """
            ã‚ãªãŸã¯å³æ ¼ãªæŸ»èª­è€…ã§ã™ã€‚
            3ã¤ã®AIã®å›ç­”ã‚’æ¯”è¼ƒã—ã€æœ€ã‚‚æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            """
        
        if image:
            contents = [prompt, image]
        else:
            contents = prompt
            
        res = client.models.generate_content(
            model="gemini-2.0-flash", 
            contents=contents,
            config={"system_instruction": sys_instruction}
        )
        return res.text.strip()
    except:
        return None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºã®ã¿) ---
with st.sidebar:
    st.title("âš›ï¸ Sci-Core AI")
    st.caption("v3.2 Open Edition")
    
    # ã‹ã£ã“ã„ã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒ¼
    st.markdown("### ğŸ–¥ï¸ System Status")
    col1, col2, col3 = st.columns(3)
    col1.metric("Core A", "ğŸŸ¢")
    col2.metric("Core B", "ğŸŸ¢")
    col3.metric("Core C", "ğŸŸ¢")
    st.success("ğŸ‘¨â€âš–ï¸ Judge System: Active")
    
    st.markdown("---")
    st.info("ğŸ“¸ ç”»åƒè§£æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ­è¼‰")
    
    if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚’æ¶ˆå»", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("ğŸ‘ï¸ Sci-Core Lens")
st.markdown("#### ç”»åƒè§£æ Ã— è¶…é«˜ç²¾åº¦è¨ˆç®—")

# å±¥æ­´è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if "image" in message:
            st.image(message["image"], width=250)
        st.markdown(message["content"])
        if "details" in message:
            with st.expander("ğŸ” è§£æãƒ—ãƒ­ã‚»ã‚¹"):
                st.markdown(message["details"])

# --- å…¥åŠ›ã‚¨ãƒªã‚¢ ---
# ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("ğŸ“¸ ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (æ•°å¼ã€ã‚°ãƒ©ãƒ•ã€å›³ãªã©)", type=["jpg", "png", "jpeg"])
# è³ªå•å…¥åŠ›
question = st.chat_input("è³ªå•ã‚’å…¥åŠ› (ä¾‹: ã“ã®æ•°å¼ã‚’è§£ã„ã¦)...")

if question:
    # ç”»åƒã®å‡¦ç†
    image = None
    if uploaded_file:
        image = Image.open(uploaded_file)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ã‚’è¡¨ç¤º
    with st.chat_message("user"):
        if image:
            st.image(image, width=250)
        st.markdown(question)
    
    # å±¥æ­´ã«ä¿å­˜
    msg_data = {"role": "user", "content": question}
    if image: msg_data["image"] = image
    st.session_state.messages.append(msg_data)

    # AIã®å‡¦ç†
    with st.chat_message("assistant"):
        status = st.empty()
        status.info("âš¡ 3ã¤ã®AIãŒè§£æä¸­...")
        
        # 1. ã‚½ãƒ«ãƒãƒ¼å®Ÿè¡Œ
        res_a = call_science_model(client, question, image, "solver")
        res_b = call_science_model(client, question, image, "solver")
        res_c = call_science_model(client, question, image, "solver")
        
        ans_a = res_a if res_a else "Error"
        ans_b = res_b if res_b else "Error"
        ans_c = res_c if res_c else "Error"
        
        # 2. æŸ»èª­
        status.info("ğŸ‘¨â€âš–ï¸ æŸ»èª­ä¸­...")
        
        log_text = f"**A:** {ans_a}\n**B:** {ans_b}\n**C:** {ans_c}"

        judge_prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
        ã€å›ç­”Aã€‘{ans_a}
        ã€å›ç­”Bã€‘{ans_b}
        ã€å›ç­”Cã€‘{ans_c}
        
        ä¸Šè¨˜ã‚’çµ±åˆã—ã€æ­£ã—ã„å›ç­”ã‚’ä½œæˆã›ã‚ˆã€‚æ•°å¼ã¯$$ã‚’ä½¿ç”¨ã›ã‚ˆã€‚
        """
        
        final_answer = call_science_model(client, judge_prompt, None, "judge")
        
        if final_answer:
            status.empty()
            st.markdown(final_answer)
            st.session_state.messages.append({
                "role": "assistant", 
                "content": final_answer, 
                "details": log_text
            })
        else:
            status.error("è§£æå¤±æ•—")
