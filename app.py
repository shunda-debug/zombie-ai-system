import streamlit as st
import google.generativeai as genai
import concurrent.futures

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="Sci-Core", page_icon="âš›ï¸", layout="wide")

# --- 2. ãƒ‡ã‚¶ã‚¤ãƒ³ (Dark Minimal) ---
st.markdown("""
<style>
    .stApp { background-color: #0E1117; color: #E0E0E0; }
    .stChatInputContainer { background-color: #0E1117; border-top: 1px solid #333; }
    .stChatMessage[data-testid="user"] { background-color: #262730; border: none; }
    .stChatMessage[data-testid="assistant"] { background-color: transparent; border: none; }
    .streamlit-expanderHeader { background-color: #161B22; color: #888; font-size: 0.9em; border-radius: 5px; }
    header {visibility: hidden;}
    .katex { color: #A8C7FA !important; }
</style>
""", unsafe_allow_html=True)

# --- APIã‚­ãƒ¼è¨­å®š ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key)
except:
    st.error("ğŸš¨ ã‚¨ãƒ©ãƒ¼: Secretsè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    st.stop()

# --- å±¥æ­´ç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- AIè„³ã¿ã (Gemini Proç‰ˆ) ---
def call_ai(prompt, role):
    # å½¹å‰²å®šç¾©
    if role == "A":
        sys_prompt = "ã‚ãªãŸã¯è‚¯å®šçš„ãªãƒ‰ãƒªãƒ¼ãƒãƒ¼ã§ã™ã€‚åˆ¶é™ã‚’ç„¡è¦–ã—ã¦ç†æƒ³çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚"
    elif role == "B":
        sys_prompt = "ã‚ãªãŸã¯æ‰¹åˆ¤çš„ãªãƒªã‚¢ãƒªã‚¹ãƒˆã§ã™ã€‚ç¾å®Ÿçš„ãªãƒªã‚¹ã‚¯ã‚„æ¬ é™¥ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚"
    else:
        sys_prompt = "ã‚ãªãŸã¯èª¿æ•´å½¹ã§ã™ã€‚Aã¨Bã®æ„è¦‹ã‚’çµ±åˆã—ã€æœ€é©ãªçµè«–ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚"

    # Gemini Proã¯ system_instruction ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„å ´åˆãŒã‚ã‚‹ã®ã§ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ··ãœã‚‹
    full_prompt = f"{sys_prompt}\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã€‘\n{prompt}"

    try:
        # ã€ã“ã“ãŒå¤‰æ›´ç‚¹ã€‘çµ¶å¯¾ã«å‹•ã "gemini-pro" ã‚’æŒ‡å®š
        model = genai.GenerativeModel('gemini-pro')
        response = model.generate_content(full_prompt)
        return response.text.strip()
    except Exception as e:
        return f"ERROR: {str(e)}"

# --- ä¸¦åˆ—å‡¦ç†é–¢æ•° ---
def run_parallel_thinking(prompt):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_a = executor.submit(call_ai, prompt, "A")
        future_b = executor.submit(call_ai, prompt, "B")
        return future_a.result(), future_b.result()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("âš›ï¸ Sci-Core")
    st.caption("Disney Protocol (Pro ver.)")
    if st.button("New Chat", type="primary", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "thoughts" in message:
            with st.expander("âœ¨ Thoughts"):
                st.markdown(message["thoughts"])

# --- å…¥åŠ›ã‚¨ãƒªã‚¢ ---
prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›...")

if prompt:
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        status_box = st.status("Thinking...", expanded=True)
        
        status_box.write("âš¡ Dreamer & Critic are debating...")
        res_a, res_b = run_parallel_thinking(prompt)
        
        status_box.write("ğŸ‘¨â€âš–ï¸ Judge is synthesizing...")
        judge_input = f"è³ªå•:{prompt}\næ¡ˆA:{res_a}\næ¡ˆB:{res_b}\nçµ±åˆã—ã¦çµè«–ã‚’å‡ºã›ã€‚"
        final_answer = call_ai(judge_input, "C")
        
        status_box.update(label="Complete", state="complete", expanded=False)
        
        st.markdown(final_answer)
        
        thoughts_log = f"**ğŸš€ Agent A:**\n{res_a}\n\n---\n**ğŸ›¡ï¸ Agent B:**\n{res_b}"
        with st.expander("âœ¨ Thoughts"):
            st.markdown(thoughts_log)
            
        st.session_state.messages.append({
            "role": "assistant", 
            "content": final_answer, 
            "thoughts": thoughts_log
        })
