import streamlit as st
import time
import re # æ­£è¦è¡¨ç¾ã‚’ä½¿ã†ãŸã‚ã«å¿…è¦
from google import genai

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ‡ã‚¶ã‚¤ãƒ³æ³¨å…¥ ---
st.set_page_config(page_title="Sci-Core AI", page_icon="âš›ï¸", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #0E1117; color: #FAFAFA; }
    .stChatMessage { background-color: #161B22; border: 1px solid #30363D; border-radius: 10px; padding: 15px; }
    .katex { font-size: 1.2em !important; color: #58A6FF !important; }
    [data-testid="stSidebar"] { background-color: #010409; border-right: 1px solid #30363D; }
    .stButton button { background-color: #238636; color: white; border-radius: 5px; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

# --- APIã‚­ãƒ¼ ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except:
    st.error("ğŸš¨ APIã‚­ãƒ¼è¨­å®šãŒå¿…è¦ã§ã™")
    st.stop()

client = genai.Client(api_key=api_key)

# --- 2. ç†ç³»ç‰¹åŒ–ã®è„³ã¿ãï¼ˆä¿®æ­£ç‰ˆï¼‰ ---
def call_science_model(client, prompt, role="solver"):
    try:
        if role == "solver":
            # â–¼â–¼â–¼ ã“ã“ã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆalignç’°å¢ƒã®ç¦æ­¢ï¼‰ â–¼â–¼â–¼
            sys_instruction = """
            ã‚ãªãŸã¯ä¸–ç•Œæœ€é«˜å³°ã®ç§‘å­¦æŠ€è¡“è¨ˆç®—AIã§ã™ã€‚
            
            ã€é‡è¦ï¼šæ•°å¼è¡¨ç¤ºãƒ«ãƒ¼ãƒ«ã€‘
            Streamlitã§è¡¨ç¤ºã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å³å®ˆã›ã‚ˆï¼š
            1. æ•°å¼ã¯å¿…ãš `$$` ã§å›²ã‚€ã“ã¨ã€‚ï¼ˆä¾‹: $$ x^2 $$ï¼‰
            2. `\\begin{align}` ã‚„ `\\begin{equation}` ãªã©ã®ç’°å¢ƒå®šç¾©ã¯**çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã“ã¨**ã€‚
            3. è¤‡æ•°è¡Œã®æ•°å¼ã‚’æ›¸ããŸã„å ´åˆã¯ã€`$$` ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆ†ã‘ã¦æ›¸ãã“ã¨ã€‚
            
            ã€è¨ˆç®—ãƒ«ãƒ¼ãƒ«ã€‘
            - æš—ç®—ç¦æ­¢ã€‚é€”ä¸­å¼ã‚’ä¸å¯§ã«æ›¸ãã€‚
            - å˜ä½ã‚’æ­£ç¢ºã«è¨˜è¿°ã™ã‚‹ã€‚
            """
        else: # Judge
            sys_instruction = """
            ã‚ãªãŸã¯å³æ ¼ãªæ•°å­¦æŸ»èª­è€…ã§ã™ã€‚
            3ã¤ã®å›ç­”ã‚’æ¯”è¼ƒã—ã€æœ€ã‚‚æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            
            ã€è¡¨ç¤ºãƒ«ãƒ¼ãƒ«ã€‘
            - `\\begin{align}` ã¯ä½¿ç”¨ç¦æ­¢ã€‚
            - ã™ã¹ã¦ã®æ•°å¼ã¯ `$$` ã¾ãŸã¯ `$` ã§å›²ã‚€ã“ã¨ã€‚
            """
        
        res = client.models.generate_content(
            model="gemini-2.0-flash", 
            contents=prompt,
            config={"system_instruction": sys_instruction}
        )
        return res.text.strip()
    except:
        return None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("âš›ï¸ Sci-Core AI")
    st.caption("v2.1 Display Fixed")
    
    col1, col2, col3 = st.columns(3)
    col1.metric("Solver A", "ON")
    col2.metric("Solver B", "ON")
    col3.metric("Solver C", "ON")
    
    st.markdown("---")
    if st.button("ğŸ—‘ï¸ é»’æ¿ã‚’æ¶ˆã™", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("âš›ï¸ Sci-Core Solver")
st.markdown("#### ç©¶æ¥µã®è¨ˆç®—ç²¾åº¦ã¨ã€ç¾ã—ã„æ•°å¼è¡¨ç¤ºã€‚")

if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "details" in message:
            with st.expander("ğŸ” è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã‚‹"):
                st.markdown(message["details"])

# è³ªå•å…¥åŠ›
question = st.chat_input("æ•°å¼ã€ç‰©ç†æ³•å‰‡ã€è¨ˆç®—å•é¡Œã‚’å…¥åŠ›...")

if question:
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state.messages.append({"role": "user", "content": question})

    with st.chat_message("assistant"):
        status = st.empty()
        status.info("âš¡ 3ã¤ã®AIè„³ãŒä¸¦åˆ—æ¼”ç®—ä¸­...")
        
        # 1. ã‚½ãƒ«ãƒãƒ¼å®Ÿè¡Œ
        res_a = call_science_model(client, question, "solver")
        res_b = call_science_model(client, question, "solver")
        res_c = call_science_model(client, question, "solver")
        
        ans_a = res_a if res_a else "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        ans_b = res_b if res_b else "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        ans_c = res_c if res_c else "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        
        # 2. æŸ»èª­
        status.info("ğŸ‘¨â€âš–ï¸ æŸ»èª­è€…ãŒæ•°å¼ã‚’æ•´å½¢ãƒ»æ¤œç®—ä¸­...")
        
        log_text = f"""
        **Solver A:**
        {ans_a}
        
        **Solver B:**
        {ans_b}
        
        **Solver C:**
        {ans_c}
        """

        # 3. æœ€çµ‚å›ç­”ç”Ÿæˆ
        judge_prompt = f"""
        ã€å•é¡Œã€‘{question}
        ã€è§£æ³•Aã€‘{ans_a}
        ã€è§£æ³•Bã€‘{ans_b}
        ã€è§£æ³•Cã€‘{ans_c}
        
        ä¸Šè¨˜ã‚’çµ±åˆã—ã€æ­£ã—ã„è¨ˆç®—çµæœã‚’å›ç­”ã—ã¦ãã ã•ã„ã€‚
        æ•°å¼ã¯å¿…ãš `$$` ã§å›²ã¿ã€alignç’°å¢ƒã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚
        """
        
        final_answer = call_science_model(client, judge_prompt, "judge")
        
        if final_answer:
            status.empty()
            st.markdown(final_answer)
            st.session_state.messages.append({
                "role": "assistant", 
                "content": final_answer, 
                "details": log_text
            })
        else:
            status.error("ğŸ’€ è¨ˆç®—å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
