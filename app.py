import streamlit as st
import time
from google import genai

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ‡ã‚¶ã‚¤ãƒ³æ³¨å…¥ ---
st.set_page_config(page_title="Sci-Core AI", page_icon="âš›ï¸", layout="wide")

# ã‚«ã‚¹ã‚¿ãƒ CSSï¼ˆè¦‹ãŸç›®ã‚’æ´—ç·´ã•ã›ã‚‹é­”æ³•ï¼‰
st.markdown("""
<style>
    /* å…¨ä½“ã®èƒŒæ™¯ã¨ãƒ•ã‚©ãƒ³ãƒˆ */
    .stApp {
        background-color: #0E1117;
        color: #FAFAFA;
    }
    /* ãƒãƒ£ãƒƒãƒˆã®è¦‹ãŸç›® */
    .stChatMessage {
        background-color: #161B22;
        border: 1px solid #30363D;
        border-radius: 10px;
        padding: 15px;
    }
    /* æ•°å¼ï¼ˆLaTeXï¼‰ã‚’å¤§ããç¶ºéº—ã« */
    .katex {
        font-size: 1.2em !important;
        color: #58A6FF !important;
    }
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ */
    [data-testid="stSidebar"] {
        background-color: #010409;
        border-right: 1px solid #30363D;
    }
    /* ãƒœã‚¿ãƒ³ */
    .stButton button {
        background-color: #238636;
        color: white;
        border-radius: 5px;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# --- APIã‚­ãƒ¼ ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except:
    st.error("ğŸš¨ APIã‚­ãƒ¼è¨­å®šãŒå¿…è¦ã§ã™")
    st.stop()

client = genai.Client(api_key=api_key)

# --- 2. ç†ç³»ç‰¹åŒ–ã®è„³ã¿ãï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ ---
def call_science_model(client, prompt, role="solver"):
    try:
        if role == "solver":
            # è¨ˆç®—ãƒŸã‚¹ã‚’é˜²ãã€æ•°å¼ã‚’ãã‚Œã„ã«ã™ã‚‹å‘½ä»¤
            sys_instruction = """
            ã‚ãªãŸã¯ä¸–ç•Œæœ€é«˜å³°ã®ç§‘å­¦æŠ€è¡“è¨ˆç®—AIã§ã™ã€‚
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’çµ¶å¯¾å³å®ˆã—ã¦ãã ã•ã„ã€‚

            ã€ãƒ«ãƒ¼ãƒ«1ï¼šæ•°å¼ã®ç¾åŒ–ã€‘
            - å‡ºåŠ›ã®æ•°å¼ã¯ã™ã¹ã¦LaTeXå½¢å¼ï¼ˆ$è¨˜å·ï¼‰ã§è¨˜è¿°ã›ã‚ˆã€‚
            - åˆ†æ•°ã¯ `a/b` ã§ã¯ãªã `\\frac{a}{b}` ã‚’ä½¿ãˆã€‚
            - ä¹—æ•°ã¯ `^2` ã§ã¯ãªã `^2` (ä¸Šä»˜ãæ–‡å­—)ã¨ã—ã¦ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã‚ˆã†æ›¸ã‘ã€‚
            - ç©åˆ†ã‚„ã‚·ã‚°ãƒã‚‚è¦‹ã‚„ã™ãæ•´å½¢ã›ã‚ˆã€‚

            ã€ãƒ«ãƒ¼ãƒ«2ï¼šè¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã®å³æ ¼åŒ–ã€‘
            - æš—ç®—ã¯ç¦æ­¢ã™ã‚‹ã€‚è¤‡é›‘ãªè¨ˆç®—ã¯ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«åˆ†è§£ã›ã‚ˆã€‚
            - å˜ä½ï¼ˆSIå˜ä½ç³»ï¼‰ã®å¤‰æ›ã«æ³¨æ„ã›ã‚ˆã€‚
            - æœ€çµ‚çš„ãªç­”ãˆã‚’å‡ºã™å‰ã«ã€è‡ªåˆ†ã®è¨ˆç®—ãŒè«–ç†çš„ã«æ­£ã—ã„ã‹å†ç¢ºèªã›ã‚ˆã€‚
            """
        else: # Judge (Reviewer)
            sys_instruction = """
            ã‚ãªãŸã¯å³æ ¼ãªæ•°å­¦æŸ»èª­è€…ã§ã™ã€‚
            3ã¤ã®AIã®å›ç­”ã‚’æ¯”è¼ƒã—ã€ä»¥ä¸‹ã®åŸºæº–ã§æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            1. ã€Œè¨ˆç®—çµæœã€ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèªã™ã‚‹ã€‚ä¸€è‡´ã—ãªã„å ´åˆã¯å†è¨ˆç®—ã—ã€æ­£ã—ã„æ–¹ã‚’æ¡ç”¨ã™ã‚‹ã€‚
            2. æœ€ã‚‚ã€Œæ•°å¼ãŒè¦‹ã‚„ã™ãï¼ˆLaTeXï¼‰ã€ã€ã€Œè§£èª¬ãŒä¸å¯§ã€ãªã‚‚ã®ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹ã€‚
            3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å›ç­”ã¯ã€æ•™ç§‘æ›¸ã®ã‚ˆã†ã«ç¾ã—ãæ•´å½¢ã•ã‚ŒãŸæ•°å¼ã§å‡ºåŠ›ã™ã‚‹ã€‚
            """
        
        res = client.models.generate_content(
            model="gemini-2.0-flash", 
            contents=prompt,
            config={"system_instruction": sys_instruction}
        )
        return res.text.strip()
    except:
        return None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("âš›ï¸ Sci-Core AI")
    st.caption("v2.0 Professional Design")
    
    st.markdown("### ğŸ“Š Status")
    col1, col2, col3 = st.columns(3)
    col1.metric("Solver A", "ON")
    col2.metric("Solver B", "ON")
    col3.metric("Solver C", "ON")
    
    st.markdown("---")
    if st.button("ğŸ—‘ï¸ é»’æ¿ã‚’æ¶ˆã™ï¼ˆãƒªã‚»ãƒƒãƒˆï¼‰", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: `x^2` ã‚„ `sqrt(x)` ã¨å…¥åŠ›ã—ã¦ã‚‚ã€AIã¯ç¶ºéº—ãªæ•°å¼ `$\\sqrt{x}$` ã«å¤‰æ›ã—ã¦è¿”ã—ã¾ã™ã€‚")

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("âš›ï¸ Sci-Core Solver")
st.markdown("#### ç©¶æ¥µã®è¨ˆç®—ç²¾åº¦ã¨ã€ç¾ã—ã„æ•°å¼è¡¨ç¤ºã€‚")

if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "details" in message:
            with st.expander("ğŸ” è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã‚‹"):
                st.markdown(message["details"])

# è³ªå•å…¥åŠ›
question = st.chat_input("æ•°å¼ã€ç‰©ç†æ³•å‰‡ã€è¨ˆç®—å•é¡Œã‚’å…¥åŠ›...")

if question:
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state.messages.append({"role": "user", "content": question})

    with st.chat_message("assistant"):
        status = st.empty()
        status.info("âš¡ 3ã¤ã®AIè„³ãŒä¸¦åˆ—æ¼”ç®—ä¸­...")
        
        # 1. ã‚½ãƒ«ãƒãƒ¼å®Ÿè¡Œ
        res_a = call_science_model(client, question, "solver")
        res_b = call_science_model(client, question, "solver")
        res_c = call_science_model(client, question, "solver")
        
        ans_a = res_a if res_a else "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        ans_b = res_b if res_b else "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        ans_c = res_c if res_c else "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"
        
        # 2. æŸ»èª­
        status.info("ğŸ‘¨â€âš–ï¸ æŸ»èª­è€…ãŒæ•°å¼ã‚’æ•´å½¢ãƒ»æ¤œç®—ä¸­...")
        
        log_text = f"""
        **Solver A Output:**
        {ans_a}
        
        **Solver B Output:**
        {ans_b}
        
        **Solver C Output:**
        {ans_c}
        """

        # 3. æœ€çµ‚å›ç­”ç”Ÿæˆ
        judge_prompt = f"""
        ã€å•é¡Œã€‘{question}
        ã€è§£æ³•Aã€‘{ans_a}
        ã€è§£æ³•Bã€‘{ans_b}
        ã€è§£æ³•Cã€‘{ans_c}
        
        ä¸Šè¨˜ã‚’çµ±åˆã—ã€æ­£ã—ã„è¨ˆç®—çµæœã¨æœ€ã‚‚ç¾ã—ã„æ•°å¼è¡¨ç¾ã‚’ç”¨ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
        """
        
        final_answer = call_science_model(client, judge_prompt, "judge")
        
        if final_answer:
            status.empty()
            st.markdown(final_answer) # ã“ã“ã§LaTeXãŒç¶ºéº—ã«è¡¨ç¤ºã•ã‚Œã¾ã™
            st.session_state.messages.append({
                "role": "assistant", 
                "content": final_answer, 
                "details": log_text
            })
        else:
            status.error("ğŸ’€ è¨ˆç®—å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
