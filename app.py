import streamlit as st
import google.generativeai as genai
import concurrent.futures

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="Sci-Core", page_icon="âš›ï¸", layout="wide")

# --- 2. ãƒ‡ã‚¶ã‚¤ãƒ³ (Disney Protocol / Dark Minimal) ---
st.markdown("""
<style>
    .stApp { background-color: #0E1117; color: #E0E0E0; }
    .stChatInputContainer { background-color: #0E1117; border-top: 1px solid #333; }
    .stChatMessage[data-testid="user"] { background-color: #262730; border: none; }
    .stChatMessage[data-testid="assistant"] { background-color: transparent; border: none; }
    .streamlit-expanderHeader { background-color: #161B22; color: #888; font-size: 0.9em; border-radius: 5px; }
    header {visibility: hidden;}
    .katex { color: #A8C7FA !important; }
</style>
""", unsafe_allow_html=True)

# --- APIã‚­ãƒ¼è¨­å®š (å®‰å®šç‰ˆ) ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key) # ã“ã“ãŒå¤‰æ›´ç‚¹ï¼
except:
    st.error("ğŸš¨ ã‚¨ãƒ©ãƒ¼: Secretsã«APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

# --- å±¥æ­´ç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- AIè„³ã¿ã (å®‰å®šç‰ˆ: google-generativeai) ---
def call_ai(prompt, role):
    try:
        # ãƒ¢ãƒ‡ãƒ«å®šç¾© (gemini-1.5-flash)
        # å®‰å®šç‰ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã‚‰ã“ã®åå‰ã§ç¢ºå®Ÿã«å‹•ãã¾ã™
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # å½¹å‰²ã”ã¨ã®ã‚·ã‚¹ãƒ†ãƒ å‘½ä»¤
        if role == "A": # Dreamer
            sys_prompt = "ã‚ãªãŸã¯è‚¯å®šçš„ãªãƒ‰ãƒªãƒ¼ãƒãƒ¼ã§ã™ã€‚åˆ¶é™ã‚’ç„¡è¦–ã—ã¦ç†æƒ³çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚"
        elif role == "B": # Realist
            sys_prompt = "ã‚ãªãŸã¯æ‰¹åˆ¤çš„ãªãƒªã‚¢ãƒªã‚¹ãƒˆã§ã™ã€‚ç¾å®Ÿçš„ãªãƒªã‚¹ã‚¯ã‚„æ¬ é™¥ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚"
        else: # C: Judge
            sys_prompt = "ã‚ãªãŸã¯èª¿æ•´å½¹ã§ã™ã€‚Aã¨Bã®æ„è¦‹ã‚’çµ±åˆã—ã€æœ€é©ãªçµè«–ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚"
        
        # å®‰å®šç‰ˆã§ã®å‘¼ã³å‡ºã—æ–¹
        # system_instructionã¯ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆæ™‚ã«æ¸¡ã™ã®ãŒä¸€èˆ¬çš„ã§ã™ãŒã€
        # ç°¡æ˜“åŒ–ã®ãŸã‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«é€£çµã—ã¦æ¸¡ã—ã¾ã™ï¼ˆã“ã‚ŒãŒä¸€ç•ªã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã„ï¼‰
        full_prompt = f"{sys_prompt}\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã€‘\n{prompt}"
        
        response = model.generate_content(full_prompt)
        return response.text.strip()
        
    except Exception as e:
        return f"ğŸš¨ ERROR: {str(e)}"

# --- ä¸¦åˆ—å‡¦ç†é–¢æ•° ---
def run_parallel_thinking(prompt):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_a = executor.submit(call_ai, prompt, "A")
        future_b = executor.submit(call_ai, prompt, "B")
        return future_a.result(), future_b.result()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.title("âš›ï¸ Sci-Core")
    st.caption("Disney Protocol v5.2 Stable")
    st.markdown("---")
    if st.button("New Chat", type="primary", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "thoughts" in message:
            with st.expander("âœ¨ Thoughts (Process A vs B)"):
                st.markdown(message["thoughts"])

# --- å…¥åŠ›ã‚¨ãƒªã‚¢ ---
prompt = st.chat_input("è³ªå•ã‚„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")

if prompt:
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        status_box = st.status("Thinking...", expanded=True)
        
        # 1. Aã¨B
        status_box.write("âš¡ Dreamer & Critic are debating...")
        res_a, res_b = run_parallel_thinking(prompt)
        
        # 2. C
        status_box.write("ğŸ‘¨â€âš–ï¸ Judge is synthesizing...")
        judge_input = f"è³ªå•:{prompt}\næ¡ˆA:{res_a}\næ¡ˆB:{res_b}\nçµ±åˆã—ã¦çµè«–ã‚’å‡ºã›ã€‚"
        final_answer = call_ai(judge_input, "C")
        
        status_box.update(label="Complete", state="complete", expanded=False)
        
        st.markdown(final_answer)
        
        thoughts_log = f"**ğŸš€ Agent A:**\n{res_a}\n\n---\n**ğŸ›¡ï¸ Agent B:**\n{res_b}"
        with st.expander("âœ¨ Thoughts (Process A vs B)"):
            st.markdown(thoughts_log)
            
        st.session_state.messages.append({
            "role": "assistant", 
            "content": final_answer, 
            "thoughts": thoughts_log
        })
